{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! git clone https://github.com/huggingface/diffusers.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danhearn\\.conda\\envs\\diffusertrack\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\danhearn\\.conda\\envs\\diffusertrack\\Lib\\site-packages\\kornia\\feature\\lightglue.py:44: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)\n",
      "c:\\Users\\danhearn\\.conda\\envs\\diffusertrack\\Lib\\site-packages\\pydub\\utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n",
      "unet\\diffusion_pytorch_model.safetensors not found\n",
      "Loading pipeline components...:   0%|          | 0/3 [00:00<?, ?it/s]An error occurred while trying to fetch C:\\Users\\danhearn\\.cache\\huggingface\\hub\\models--teticio--audio-diffusion-ddim-256\\snapshots\\f5606c5138496ecdcbd096a4446eb6d03ae690cb\\unet: Error no file named diffusion_pytorch_model.safetensors found in directory C:\\Users\\danhearn\\.cache\\huggingface\\hub\\models--teticio--audio-diffusion-ddim-256\\snapshots\\f5606c5138496ecdcbd096a4446eb6d03ae690cb\\unet.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "Loading pipeline components...: 100%|██████████| 3/3 [00:00<00:00, 12.18it/s]\n",
      "An error occurred while trying to fetch teticio/audio-diffusion-ddim-256: teticio/audio-diffusion-ddim-256 does not appear to have a file named diffusion_pytorch_model.safetensors.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import threading\n",
    "from librosa.beat import beat_track\n",
    "from pythonosc import dispatcher, osc_server\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from diffusers import DiffusionPipeline, DDIMScheduler, AudioDiffusionPipeline\n",
    "from diffusers_local import UNet2DModel as UNet2DModel_local\n",
    "import soundfile as sf\n",
    "from CreateDataset import AudioProcessor\n",
    "from NetworkBending import NetworkBending\n",
    "import time\n",
    "\n",
    "osc_lock = threading.Lock()\n",
    "\n",
    "NB = NetworkBending()\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "audio_diffusion = DiffusionPipeline.from_pretrained(\"teticio/audio-diffusion-ddim-256\")\n",
    "# apply the custom unet model so we can Networkbend the audio\n",
    "audio_diffusion.unet = UNet2DModel_local.from_pretrained(\"teticio/audio-diffusion-ddim-256\", subfolder=\"unet\", network_bending=NB)\n",
    "\n",
    "#set the scheduler to DDIM\n",
    "audio_diffusion.scheduler = DDIMScheduler.from_pretrained(\"teticio/audio-diffusion-ddim-256\", subfolder=\"scheduler\")\n",
    "audio_diffusion.to(device)\n",
    "\n",
    "#ds = load_dataset('teticio/audio-diffusion-256')\n",
    "#generator=torch.Generator(device=\"cpu\").manual_seed(42)\n",
    "\n",
    "alpha = 0\n",
    "latent1 = 0\n",
    "latent2 = 1\n",
    "diffusion_steps = 10\n",
    "make_loop = 0\n",
    "generate = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danhearn\\AppData\\Local\\Temp\\ipykernel_35904\\96833784.py:27: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  vocoder.load_state_dict(torch.load(vocoder_param_fp, map_location=torch.device(device_name)), strict=False)\n"
     ]
    }
   ],
   "source": [
    "from melgan.modules import Generator_melgan\n",
    "import yaml\n",
    "\n",
    "\n",
    "MELGAN_MODEL_NAME = \"best_netG.pt\"\n",
    "\n",
    "def read_yaml(filepath):\n",
    "    with open(filepath, 'r') as file:\n",
    "        return yaml.safe_load(file)\n",
    "\n",
    "# Load a MelGAN model\n",
    "def load_vocoder(device_name):\n",
    "    feat_dim = 80\n",
    "    mean_fp = f'data/drumbeats_1bar/mean.mel.npy'\n",
    "    std_fp = f'data/drumbeats_1bar/std.mel.npy'\n",
    "    v_mean = torch.from_numpy(np.load(mean_fp)).float().view(1, feat_dim, 1).to(device_name)\n",
    "    v_std = torch.from_numpy(np.load(std_fp)).float().view(1, feat_dim, 1).to(device_name)\n",
    "\n",
    "    n_mel_channels = 80\n",
    "    ngf = 32\n",
    "    n_residual_layers = 3\n",
    "\n",
    "    vocoder = Generator_melgan(n_mel_channels, ngf, n_residual_layers).to(device_name)\n",
    "    vocoder.eval()\n",
    "\n",
    "    vocoder_param_fp = f'melgan/{MELGAN_MODEL_NAME}'\n",
    "    vocoder.load_state_dict(torch.load(vocoder_param_fp, map_location=torch.device(device_name)), strict=False)\n",
    "\n",
    "    return vocoder, v_mean, v_std\n",
    "\n",
    "VOCODER, V_MEAN, V_STD = load_vocoder(device)\n",
    "\n",
    "def vocode(sample, vocoder=VOCODER, v_mean=V_MEAN, v_std=V_STD):\n",
    "    de_norm = sample.squeeze(0) * v_std + v_mean\n",
    "    audio_output = vocoder(de_norm)\n",
    "    return audio_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "LibsndfileError",
     "evalue": "Error opening 'mel_gan.wav': Format not recognised.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLibsndfileError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 40\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Ensure audio_output is float32\u001b[39;00m\n\u001b[0;32m     38\u001b[0m audio_output \u001b[38;5;241m=\u001b[39m audio_output\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m---> 40\u001b[0m \u001b[43msf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmel_gan.wav\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m22050\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\danhearn\\.conda\\envs\\diffusertrack\\Lib\\site-packages\\soundfile.py:343\u001b[0m, in \u001b[0;36mwrite\u001b[1;34m(file, data, samplerate, subtype, endian, format, closefd)\u001b[0m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    342\u001b[0m     channels \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m--> 343\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mSoundFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplerate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    344\u001b[0m \u001b[43m               \u001b[49m\u001b[43msubtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendian\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosefd\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    345\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(data)\n",
      "File \u001b[1;32mc:\\Users\\danhearn\\.conda\\envs\\diffusertrack\\Lib\\site-packages\\soundfile.py:658\u001b[0m, in \u001b[0;36mSoundFile.__init__\u001b[1;34m(self, file, mode, samplerate, channels, subtype, endian, format, closefd)\u001b[0m\n\u001b[0;32m    655\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m=\u001b[39m mode\n\u001b[0;32m    656\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info \u001b[38;5;241m=\u001b[39m _create_info_struct(file, mode, samplerate, channels,\n\u001b[0;32m    657\u001b[0m                                  \u001b[38;5;28mformat\u001b[39m, subtype, endian)\n\u001b[1;32m--> 658\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode_int\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosefd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    659\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mset\u001b[39m(mode)\u001b[38;5;241m.\u001b[39missuperset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseekable():\n\u001b[0;32m    660\u001b[0m     \u001b[38;5;66;03m# Move write position to 0 (like in Python file objects)\u001b[39;00m\n\u001b[0;32m    661\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\danhearn\\.conda\\envs\\diffusertrack\\Lib\\site-packages\\soundfile.py:1216\u001b[0m, in \u001b[0;36mSoundFile._open\u001b[1;34m(self, file, mode_int, closefd)\u001b[0m\n\u001b[0;32m   1213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file_ptr \u001b[38;5;241m==\u001b[39m _ffi\u001b[38;5;241m.\u001b[39mNULL:\n\u001b[0;32m   1214\u001b[0m     \u001b[38;5;66;03m# get the actual error code\u001b[39;00m\n\u001b[0;32m   1215\u001b[0m     err \u001b[38;5;241m=\u001b[39m _snd\u001b[38;5;241m.\u001b[39msf_error(file_ptr)\n\u001b[1;32m-> 1216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LibsndfileError(err, prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError opening \u001b[39m\u001b[38;5;132;01m{0!r}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname))\n\u001b[0;32m   1217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode_int \u001b[38;5;241m==\u001b[39m _snd\u001b[38;5;241m.\u001b[39mSFM_WRITE:\n\u001b[0;32m   1218\u001b[0m     \u001b[38;5;66;03m# Due to a bug in libsndfile version <= 1.0.25, frames != 0\u001b[39;00m\n\u001b[0;32m   1219\u001b[0m     \u001b[38;5;66;03m# when opening a named pipe in SFM_WRITE mode.\u001b[39;00m\n\u001b[0;32m   1220\u001b[0m     \u001b[38;5;66;03m# See http://github.com/erikd/libsndfile/issues/77.\u001b[39;00m\n\u001b[0;32m   1221\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mframes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[1;31mLibsndfileError\u001b[0m: Error opening 'mel_gan.wav': Format not recognised."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Load the PNG image as a normalized tensor\n",
    "def load_image_as_tensor(image_path, device, target_dim=(80, 256)):\n",
    "    # Load image and convert to grayscale if necessary\n",
    "    image = Image.open(image_path).convert('L')  # Convert to grayscale\n",
    "\n",
    "    # Resize image to the target dimensions (e.g., 80x256)\n",
    "    image = image.resize(target_dim, Image.LANCZOS)\n",
    "    \n",
    "    # Convert image to numpy array, normalize it between -1 and 1\n",
    "    image_np = np.array(image).astype(np.float32)\n",
    "    image_np = (image_np / 255.0) * 2 - 1  # Normalize to match vocoder's expected input range\n",
    "\n",
    "    # Convert numpy array to a torch tensor and add batch and channel dimensions\n",
    "    image_tensor = torch.tensor(image_np).unsqueeze(0).unsqueeze(0).to(device)\n",
    "    \n",
    "    return image_tensor\n",
    "\n",
    "# Load and process the image\n",
    "image_path = 'generated_image.png'  # Path to your PNG image\n",
    "image_tensor = load_image_as_tensor(image_path, device)\n",
    "\n",
    "# Ensure the tensor has the correct dimensions\n",
    "if image_tensor.shape[2] != V_MEAN.shape[2]:\n",
    "    image_tensor = torch.nn.functional.interpolate(image_tensor, size=(V_MEAN.shape[2], image_tensor.shape[3]), mode='bilinear', align_corners=False)\n",
    "\n",
    "# Run the vocoder to generate audio\n",
    "audio_output = vocode(image_tensor).squeeze(0).cpu().detach().numpy()\n",
    "\n",
    "# Ensure audio_output is 2D (samples, channels)\n",
    "if audio_output.ndim == 1:\n",
    "    audio_output = audio_output[:, np.newaxis]\n",
    "\n",
    "# Ensure audio_output is float32\n",
    "audio_output = audio_output.astype(np.float32)\n",
    "\n",
    "sf.write(\"mel_gan.wav\", audio_output, 22050)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load audio and process it into 5 second spectrograms\n",
    "input_dir = r\"C:\\\\Users\\\\dhearn\\\\Music\"\n",
    "processor = AudioProcessor(input_dir)\n",
    "processor.process_directory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop_it(audio: np.ndarray,\n",
    "        sample_rate: int,\n",
    "        loops: int = 1) -> np.ndarray:\n",
    "    \"\"\"Loop audio\n",
    "\n",
    "    Args:\n",
    "        audio (np.ndarray): audio as numpy array\n",
    "        sample_rate (int): sample rate of audio\n",
    "        loops (int): number of times to loop\n",
    "\n",
    "    Returns:\n",
    "        (float, np.ndarray): sample rate and raw audio or None\n",
    "    \"\"\"\n",
    "    _, beats = beat_track(y=audio, sr=sample_rate, units='samples')\n",
    "    for beats_in_bar in [16, 12, 8, 4]:\n",
    "        if len(beats) > beats_in_bar:\n",
    "            return np.tile(audio[beats[0]:beats[beats_in_bar]], loops)\n",
    "    return None\n",
    "\n",
    "def interpolation(address, *args):\n",
    "    \n",
    "    \"\"\"Interpolate between two latents for generation\n",
    "    Args:\n",
    "        address (str): OSC address\n",
    "        *args: OSC arguments\n",
    "    \"\"\"\n",
    "\n",
    "    global alpha, latent1, latent2\n",
    "    \n",
    "    if address == \"/alpha\":\n",
    "        alpha = args[0]\n",
    "    if address == \"/latent1\" and latent2 != args[0]:\n",
    "        latent1 = int(args[0])\n",
    "    if address == \"/latent2\" and latent1 != args[0]:\n",
    "        latent2 = int(args[0])\n",
    "\n",
    "def inference(address, *args):\n",
    "\n",
    "    \"\"\"Control the inference process\n",
    "    Args:\n",
    "        address (str): OSC address\n",
    "        *args: OSC arguments\n",
    "    \"\"\"\n",
    "\n",
    "    global diffusion_steps, make_loop, generate\n",
    "\n",
    "    if address == \"/steps\":\n",
    "        diffusion_steps = int(args[0])\n",
    "    if address == \"/loop\":\n",
    "        make_loop = int(args[0])\n",
    "    if address == \"/generate\":\n",
    "        generate = int(args[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load 20 images from the default dataset\n",
    "ds = load_dataset('teticio/audio-diffusion-256')\n",
    "images = []\n",
    "\n",
    "for i in range(20):\n",
    "    images.append(random.choice(ds['train'])['image'])\n",
    "\n",
    "images[0].show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Assuming `audio_diffusion` is your Diffusion model pipeline\n",
    "encoded_images = []\n",
    "\n",
    "# Directory containing mel spectrograms\n",
    "spectrogram_dir = \"data/mel_spectrograms\"\n",
    "\n",
    "# Loop through each file in the directory\n",
    "for i, file_name in enumerate(os.listdir(spectrogram_dir)):\n",
    "    spectrogram_path = os.path.join(spectrogram_dir, file_name)\n",
    "    spectrogram_image = Image.open(spectrogram_path).convert(\"L\")\n",
    "    encoded_image = audio_diffusion.encode([spectrogram_image], steps=100)\n",
    "    encoded_images.append(encoded_image)\n",
    "    if len(encoded_images) >= 10:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for encoded_image in encoded_images:\n",
    "    plt.imshow(encoded_image.cpu().squeeze(), cmap='gray')\n",
    "    plt.axis('off')  # Hide axes\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#osc listener\n",
    "from pythonosc import dispatcher, osc_server\n",
    "\n",
    "ip=\"127.0.0.1\"\n",
    "port=9999\n",
    "\n",
    "d = dispatcher.Dispatcher()\n",
    "# Map incoming OSC messages to the osc_receive method\n",
    "d.map(\"/rotate_x_radian\",  NB.osc_receive)\n",
    "d.map(\"/rotate_y_radian\", NB.osc_receive)\n",
    "d.map(\"/rotate_z_radian\", NB.osc_receive)\n",
    "d.map(\"/rotate_x_scaling_factor\", NB.osc_receive)\n",
    "d.map(\"/rotate_y_scaling_factor\", NB.osc_receive)\n",
    "d.map(\"/rotate_z_scaling_factor\", NB.osc_receive)\n",
    "d.map(\"/scale_factor\", NB.osc_receive)\n",
    "d.map(\"/layer\", NB.osc_receive)\n",
    "d.map(\"/scale\", NB.osc_receive)\n",
    "d.map(\"/reflect\", NB.osc_receive)\n",
    "d.map(\"/erosion\", NB.osc_receive)\n",
    "d.map(\"/dilation\", NB.osc_receive)\n",
    "d.map(\"/gradient\", NB.osc_receive)\n",
    "d.map(\"/sobel\", NB.osc_receive)\n",
    "d.map(\"/add_rand_rows\", NB.osc_receive)\n",
    "d.map(\"/normalize\", NB.osc_receive)\n",
    "d.map(\"/rotate_x\", NB.osc_receive)\n",
    "d.map(\"/rotate_y\", NB.osc_receive)\n",
    "d.map(\"/rotate_z\", NB.osc_receive)\n",
    "d.map(\"/alpha\", interpolation)\n",
    "d.map(\"/latent1\", interpolation)\n",
    "d.map(\"/latent2\", interpolation)\n",
    "d.map(\"/steps\", inference)\n",
    "d.map(\"/loop\", inference)\n",
    "d.map(\"/generate\", inference)\n",
    "\n",
    "s = osc_server.ThreadingOSCUDPServer((ip, port), d)\n",
    "\n",
    "osc_thread = threading.Thread(target=s.serve_forever)\n",
    "osc_thread.daemon = True # This will allow the main program to exit even if the OSC server is still running\n",
    "osc_thread.start()\n",
    "print(\"OSC server started - listening on port 9999\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main loop for image generation\n",
    "try:\n",
    "    while True:\n",
    "        with osc_lock: \n",
    "\n",
    "            if generate == 1:\n",
    "             \n",
    "                output = audio_diffusion(steps=diffusion_steps,\n",
    "                noise=AudioDiffusionPipeline.slerp(encoded_images[latent1], encoded_images[latent2], alpha), eta=0)\n",
    "\n",
    "                output.images[0].save(\"generated_image.png\")\n",
    "                audio = output.audios[0, 0]\n",
    "\n",
    "                if make_loop == 1:\n",
    "                    loop = loop_it(audio, audio_diffusion.mel.get_sample_rate())\n",
    "                    sf.write(\"output_audio.wav\", loop, audio_diffusion.mel.get_sample_rate())\n",
    "                elif make_loop == 0:\n",
    "                    time.sleep(0.05)\n",
    "                    sf.write(\"output_audio.wav\", audio, audio_diffusion.mel.get_sample_rate())\n",
    "\n",
    "\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    s.shutdown()\n",
    "    print(\"Terminating the loop gracefully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusertrack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
