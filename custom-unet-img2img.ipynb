{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danhearn\\.conda\\envs\\diffusertrack\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\danhearn\\.conda\\envs\\diffusertrack\\Lib\\site-packages\\kornia\\feature\\lightglue.py:44: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OSC server running...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "AutoPipeline can't find a pipeline linked to AudioDiffusionPipeline for None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m osc_thread\u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Initialize the pipeline\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m pipe \u001b[38;5;241m=\u001b[39m \u001b[43mAutoPipelineForImage2Image\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mteticio/audio-diffusion-256\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfp16\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m pipe\u001b[38;5;241m.\u001b[39munet \u001b[38;5;241m=\u001b[39m UNet2DConditionModel_local\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mteticio/audio-diffusion-256\u001b[39m\u001b[38;5;124m\"\u001b[39m, subfolder\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munet\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, variant\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfp16\u001b[39m\u001b[38;5;124m\"\u001b[39m, network_bending\u001b[38;5;241m=\u001b[39mNB)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m#pipe.load_lora_weights(\"Danhearn/spectrogram\", weight_name=\"pytorch_lora_weights.safetensors\", adapter_name=\"amen\")\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\danhearn\\.conda\\envs\\diffusertrack\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\danhearn\\.conda\\envs\\diffusertrack\\Lib\\site-packages\\diffusers\\pipelines\\auto_pipeline.py:666\u001b[0m, in \u001b[0;36mAutoPipelineForImage2Image.from_pretrained\u001b[1;34m(cls, pretrained_model_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    663\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m enable_pag:\n\u001b[0;32m    664\u001b[0m         orig_class_name \u001b[38;5;241m=\u001b[39m orig_class_name\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPAGPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 666\u001b[0m image_2_image_cls \u001b[38;5;241m=\u001b[39m \u001b[43m_get_task_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mAUTO_IMAGE2IMAGE_PIPELINES_MAPPING\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morig_class_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    668\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mload_config_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[0;32m    669\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image_2_image_cls\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\danhearn\\.conda\\envs\\diffusertrack\\Lib\\site-packages\\diffusers\\pipelines\\auto_pipeline.py:210\u001b[0m, in \u001b[0;36m_get_task_class\u001b[1;34m(mapping, pipeline_class_name, throw_error_if_not_exist)\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m task_class\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m throw_error_if_not_exist:\n\u001b[1;32m--> 210\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoPipeline can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find a pipeline linked to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpipeline_class_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: AutoPipeline can't find a pipeline linked to AudioDiffusionPipeline for None"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import threading\n",
    "from diffusers import AutoPipelineForImage2Image\n",
    "from diffusers import DDIMScheduler\n",
    "from diffusers_local import UNet2DConditionModel as UNet2DConditionModel_local\n",
    "from diffusers.utils import load_image\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "from NetworkBending import NetworkBending\n",
    "import time  # for safely terminating the loop\n",
    "\n",
    "# Create a global lock for synchronizing OSC parameter updates\n",
    "osc_lock = threading.Lock()\n",
    "\n",
    "# Instantiate NetworkBending object\n",
    "NB = NetworkBending()\n",
    "\n",
    "# Start the OSC server in a background thread\n",
    "osc_thread = threading.Thread(target=NB.serve_forever)\n",
    "osc_thread.daemon = True  # Ensure the OSC thread terminates when the program ends\n",
    "osc_thread.start()\n",
    "\n",
    "# Initialize the pipeline\n",
    "pipe = AutoPipelineForImage2Image.from_pretrained(\"teticio/audio-diffusion-256\", torch_dtype=torch.float32, variant=\"fp16\", )\n",
    "pipe.unet = UNet2DConditionModel_local.from_pretrained(\"teticio/audio-diffusion-256\", subfolder=\"unet\", torch_dtype=torch.float32, variant=\"fp16\", network_bending=NB)\n",
    "#pipe.load_lora_weights(\"Danhearn/spectrogram\", weight_name=\"pytorch_lora_weights.safetensors\", adapter_name=\"amen\")\n",
    "pipe.to(\"cuda\")\n",
    "\n",
    "init_image = load_image(r\"C:\\\\Users\\\\danhearn\\Documents\\\\GitHub\\\\diffusion-models-class\\\\unit4\\\\spectrograms\\\\cw_amen17_175.png\")\n",
    "\n",
    "# pipe.scheduler = DDIMScheduler(\n",
    "#     beta_start=0.00085,\n",
    "#     beta_end=0.012,\n",
    "#     beta_schedule=\"scaled_linear\",\n",
    "#     clip_sample=False,\n",
    "#     set_alpha_to_one=False,\n",
    "#     steps_offset=1,\n",
    "#     prediction_type=\"epsilon\",\n",
    "#     num_train_timesteps=1000,\n",
    "#     timestep_spacing=\"trailing\",\n",
    "#     trained_betas=None\n",
    "# )\n",
    "\n",
    "#pipe.set_progress_bar_config(disable=True)\n",
    "\n",
    "#print(pipe.scheduler)\n",
    "#print(pipe.unet)\n",
    "\n",
    "# Main loop for image generation\n",
    "try:\n",
    "    while True:\n",
    "        with osc_lock:  # Ensure that network bending is thread-safe\n",
    "            output = pipe(prompt=\" \", image=init_image, generator=torch.Generator(device=\"cpu\").manual_seed(42), strength=0.5, guidance_scale=0, num_inference_steps=4, width=256, height=256)\n",
    "        \n",
    "        image = output.images[0]\n",
    "        clear_output(wait=True)\n",
    "        plt.imshow(image)\n",
    "        plt.axis('off')  # Hide axes\n",
    "        display(plt.gcf())\n",
    "        #time.sleep(0.1)  # Add a small sleep to prevent overwhelming CPU\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    NB.stop()\n",
    "    print(\"Terminating the loop gracefully.\")\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusertrack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
